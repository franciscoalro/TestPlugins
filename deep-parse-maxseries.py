#!/usr/bin/env python3
"""
Parse profundo do MaxSeries.one para capturar estrutura completa
- PÃ¡gina inicial
- Filmes
- SÃ©ries
- EpisÃ³dios
- Players
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:146.0) Gecko/20100101 Firefox/146.0"

def save_html(content, filename):
    """Salva HTML para anÃ¡lise"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"âœ… Salvo: {filename}")

def parse_home():
    """Parse da pÃ¡gina inicial"""
    print("\n" + "="*60)
    print("ğŸ  PÃGINA INICIAL")
    print("="*60)
    
    url = "https://www.maxseries.one"
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    response = requests.get(url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, "maxseries_home.html")
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Estrutura da pÃ¡gina
    print("\nğŸ“‹ Estrutura da Home:")
    
    # Menu principal
    menu = soup.select("nav a, .menu a, header a")
    print(f"\nğŸ”— Links do Menu ({len(menu)}):")
    for link in menu[:10]:
        href = link.get('href', '')
        text = link.get_text(strip=True)
        if text and href:
            print(f"  - {text}: {href}")
    
    # SeÃ§Ãµes de conteÃºdo
    sections = soup.select("section, .section, .content-section")
    print(f"\nğŸ“¦ SeÃ§Ãµes de ConteÃºdo: {len(sections)}")
    
    # Cards de filmes/sÃ©ries
    cards = soup.select("article.item, .item, .movie-item, .serie-item")
    print(f"\nğŸ¬ Cards de ConteÃºdo: {len(cards)}")
    
    if cards:
        print("\nğŸ“ Estrutura de um Card:")
        card = cards[0]
        print(f"  HTML: {card.prettify()[:500]}...")
        
        # Extrair informaÃ§Ãµes
        title = card.select_one("h3, .title, h2")
        link = card.select_one("a")
        image = card.select_one("img")
        year = card.select_one(".year, .data span")
        
        print(f"\n  TÃ­tulo: {title.get_text(strip=True) if title else 'N/A'}")
        print(f"  Link: {link.get('href') if link else 'N/A'}")
        print(f"  Imagem: {image.get('src') or image.get('data-src') if image else 'N/A'}")
        print(f"  Ano: {year.get_text(strip=True) if year else 'N/A'}")
    
    # Categorias
    categories = soup.select(".genres a, .category a, .sgeneros a")
    print(f"\nğŸ·ï¸ Categorias: {len(categories)}")
    for cat in categories[:5]:
        print(f"  - {cat.get_text(strip=True)}")
    
    return {
        "url": url,
        "cards_count": len(cards),
        "sections_count": len(sections)
    }

def parse_movies_page():
    """Parse da pÃ¡gina de filmes"""
    print("\n" + "="*60)
    print("ğŸ¬ PÃGINA DE FILMES")
    print("="*60)
    
    url = "https://www.maxseries.one/filmes"
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    response = requests.get(url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, "maxseries_filmes.html")
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Cards de filmes
    cards = soup.select("article.item, .item, .movie-item")
    print(f"\nğŸ¬ Total de Filmes: {len(cards)}")
    
    # PaginaÃ§Ã£o
    pagination = soup.select(".pagination a, .nav-links a")
    print(f"\nğŸ“„ PaginaÃ§Ã£o: {len(pagination)} links")
    
    # Filtros
    filters = soup.select(".filters select, .filter-select")
    print(f"\nğŸ” Filtros: {len(filters)}")
    
    # Analisar 3 filmes
    movies = []
    for i, card in enumerate(cards[:3]):
        print(f"\nğŸ“½ï¸ Filme {i+1}:")
        
        title_elem = card.select_one("h3, .title, h2")
        link_elem = card.select_one("a")
        img_elem = card.select_one("img")
        
        title = title_elem.get_text(strip=True) if title_elem else "N/A"
        link = link_elem.get('href') if link_elem else None
        img = img_elem.get('src') or img_elem.get('data-src') if img_elem else None
        
        print(f"  TÃ­tulo: {title}")
        print(f"  Link: {link}")
        print(f"  Imagem: {img}")
        
        movie = {
            "title": title,
            "link": link,
            "image": img
        }
        movies.append(movie)
    
    return movies

def parse_series_page():
    """Parse da pÃ¡gina de sÃ©ries"""
    print("\n" + "="*60)
    print("ğŸ“º PÃGINA DE SÃ‰RIES")
    print("="*60)
    
    url = "https://www.maxseries.one/series"
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    response = requests.get(url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, "maxseries_series.html")
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Cards de sÃ©ries
    cards = soup.select("article.item, .item, .serie-item")
    print(f"\nğŸ“º Total de SÃ©ries: {len(cards)}")
    
    # Analisar 3 sÃ©ries
    series = []
    for i, card in enumerate(cards[:3]):
        print(f"\nğŸ“º SÃ©rie {i+1}:")
        
        title_elem = card.select_one("h3, .title, h2")
        link_elem = card.select_one("a")
        img_elem = card.select_one("img")
        
        title = title_elem.get_text(strip=True) if title_elem else "N/A"
        link = link_elem.get('href') if link_elem else None
        img = img_elem.get('src') or img_elem.get('data-src') if img_elem else None
        
        print(f"  TÃ­tulo: {title}")
        print(f"  Link: {link}")
        
        if link:
            series.append({
                "title": title,
                "link": link,
                "image": img
            })
    
    return series

def parse_movie_detail(movie_url):
    """Parse de uma pÃ¡gina de filme"""
    print("\n" + "="*60)
    print(f"ğŸ¬ DETALHES DO FILME")
    print("="*60)
    print(f"URL: {movie_url}")
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    response = requests.get(movie_url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, "maxseries_movie_detail.html")
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # TÃ­tulo
    title = soup.select_one("h1, .title")
    print(f"\nğŸ“ TÃ­tulo: {title.get_text(strip=True) if title else 'N/A'}")
    
    # Poster
    poster = soup.select_one(".poster img, .movie-poster img")
    print(f"ğŸ–¼ï¸ Poster: {poster.get('src') if poster else 'N/A'}")
    
    # Sinopse
    plot = soup.select_one(".description, .sinopse, .plot")
    print(f"ğŸ“– Sinopse: {plot.get_text(strip=True)[:100] if plot else 'N/A'}...")
    
    # GÃªneros
    genres = soup.select(".sgeneros a, .genres a")
    print(f"ğŸ·ï¸ GÃªneros: {', '.join([g.get_text(strip=True) for g in genres])}")
    
    # Ano
    year_elem = soup.select_one(".year, .data")
    print(f"ğŸ“… Ano: {year_elem.get_text(strip=True) if year_elem else 'N/A'}")
    
    # Iframe do player
    iframes = soup.select("iframe")
    print(f"\nğŸ¥ Iframes encontrados: {len(iframes)}")
    for i, iframe in enumerate(iframes):
        src = iframe.get('src', '')
        print(f"  {i+1}. {src}")
    
    # Procurar playerthree
    playerthree_pattern = re.compile(r'https?://playerthree\.online/[^"\']+')
    playerthree_urls = playerthree_pattern.findall(html)
    print(f"\nğŸ® URLs do PlayerThree: {len(playerthree_urls)}")
    for url in playerthree_urls[:3]:
        print(f"  - {url}")
    
    return {
        "title": title.get_text(strip=True) if title else None,
        "playerthree_urls": playerthree_urls
    }

def parse_series_detail(series_url):
    """Parse de uma pÃ¡gina de sÃ©rie"""
    print("\n" + "="*60)
    print(f"ğŸ“º DETALHES DA SÃ‰RIE")
    print("="*60)
    print(f"URL: {series_url}")
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    response = requests.get(series_url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, "maxseries_series_detail.html")
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # TÃ­tulo
    title = soup.select_one("h1, .title")
    print(f"\nğŸ“ TÃ­tulo: {title.get_text(strip=True) if title else 'N/A'}")
    
    # Temporadas
    seasons = soup.select(".se-c, .seasons .se-a, #seasons .se-c")
    print(f"\nğŸ“º Temporadas: {len(seasons)}")
    
    # EpisÃ³dios
    episodes_data = []
    for season_idx, season in enumerate(seasons[:2]):  # Analisar 2 temporadas
        print(f"\nğŸ¬ Temporada {season_idx + 1}:")
        
        episodes = season.select(".episodios li, .se-a ul li, ul.episodios li")
        print(f"  EpisÃ³dios: {len(episodes)}")
        
        for ep_idx, episode in enumerate(episodes[:3]):  # Analisar 3 episÃ³dios
            ep_link = episode.select_one("a")
            ep_title = episode.select_one(".episodiotitle a, .epst")
            
            if ep_link:
                ep_url = ep_link.get('href')
                ep_name = ep_title.get_text(strip=True) if ep_title else f"EpisÃ³dio {ep_idx + 1}"
                
                print(f"    {ep_idx + 1}. {ep_name}: {ep_url}")
                
                episodes_data.append({
                    "season": season_idx + 1,
                    "episode": ep_idx + 1,
                    "title": ep_name,
                    "url": ep_url
                })
    
    # Iframe do player
    iframes = soup.select("iframe")
    print(f"\nğŸ¥ Iframes encontrados: {len(iframes)}")
    for i, iframe in enumerate(iframes):
        src = iframe.get('src', '')
        if 'playerthree' in src:
            print(f"  âœ… PlayerThree: {src}")
    
    # Procurar playerthree no HTML
    playerthree_pattern = re.compile(r'https?://playerthree\.online/[^"\']+')
    playerthree_urls = playerthree_pattern.findall(html)
    print(f"\nğŸ® URLs do PlayerThree: {len(playerthree_urls)}")
    for url in set(playerthree_urls)[:3]:
        print(f"  - {url}")
    
    return {
        "title": title.get_text(strip=True) if title else None,
        "seasons": len(seasons),
        "episodes": episodes_data,
        "playerthree_urls": list(set(playerthree_urls))
    }

def parse_playerthree(playerthree_url):
    """Parse da pÃ¡gina do PlayerThree"""
    print("\n" + "="*60)
    print(f"ğŸ® PLAYERTHREE")
    print("="*60)
    print(f"URL: {playerthree_url}")
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": "https://www.maxseries.one"
    }
    
    response = requests.get(playerthree_url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, "maxseries_playerthree.html")
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Temporadas
    seasons = soup.select(".header-navigation li[data-season-id]")
    print(f"\nğŸ“º Temporadas: {len(seasons)}")
    for season in seasons[:3]:
        season_id = season.get('data-season-id')
        season_num = season.get('data-season-number')
        season_name = season.get_text(strip=True)
        print(f"  - ID: {season_id}, NÃºmero: {season_num}, Nome: {season_name}")
    
    # Cards de episÃ³dios
    cards = soup.select(".card")
    print(f"\nğŸ¬ Cards: {len(cards)}")
    
    for card_idx, card in enumerate(cards[:2]):
        card_title = card.select_one(".card-title")
        print(f"\nğŸ“¦ Card {card_idx + 1}: {card_title.get_text(strip=True) if card_title else 'N/A'}")
        
        episodes = card.select("li[data-episode-id]")
        print(f"  EpisÃ³dios: {len(episodes)}")
        
        for ep in episodes[:3]:
            ep_id = ep.get('data-episode-id')
            ep_season = ep.get('data-season-id')
            ep_link = ep.select_one("a")
            ep_text = ep_link.get_text(strip=True) if ep_link else "N/A"
            
            print(f"    - ID: {ep_id}, Season: {ep_season}, Texto: {ep_text}")
    
    return {
        "seasons": len(seasons),
        "cards": len(cards)
    }

def parse_playerthree_episode(playerthree_url, episode_id):
    """Parse de um episÃ³dio especÃ­fico do PlayerThree"""
    print("\n" + "="*60)
    print(f"ğŸ¬ EPISÃ“DIO DO PLAYERTHREE")
    print("="*60)
    
    base_url = playerthree_url.split("/embed/")[0] if "/embed/" in playerthree_url else "https://playerthree.online"
    episode_url = f"{base_url}/episodio/{episode_id}"
    
    print(f"URL: {episode_url}")
    
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "*/*",
        "Referer": playerthree_url,
        "X-Requested-With": "XMLHttpRequest"
    }
    
    response = requests.get(episode_url, headers=headers, timeout=15)
    html = response.text
    
    save_html(html, f"maxseries_episode_{episode_id}.html")
    
    print(f"\nğŸ“„ Resposta ({len(html)} chars)")
    print(f"InÃ­cio: {html[:300]}...")
    
    # Extrair sources
    sources = []
    
    # PadrÃ£o 1: data-source
    pattern1 = re.compile(r'data-source\s*=\s*["\']([^"\']+)["\']', re.IGNORECASE)
    for match in pattern1.findall(html):
        if match.startswith("http"):
            sources.append(match)
    
    # PadrÃ£o 2: data-src
    pattern2 = re.compile(r'data-src\s*=\s*["\']([^"\']+)["\']', re.IGNORECASE)
    for match in pattern2.findall(html):
        if match.startswith("http"):
            sources.append(match)
    
    sources = list(set(sources))
    
    print(f"\nğŸ¯ Sources encontradas: {len(sources)}")
    for src in sources:
        # Identificar tipo
        if "playerembedapi" in src.lower():
            print(f"  ğŸŸ¢ PlayerEmbedAPI: {src}")
        elif "myvidplay" in src.lower():
            print(f"  ğŸŸ¡ MyVidPlay: {src}")
        elif "dood" in src.lower():
            print(f"  ğŸŸ  Dood: {src}")
        elif "megaembed" in src.lower():
            print(f"  ğŸ”´ MegaEmbed: {src}")
        elif "streamtape" in src.lower():
            print(f"  ğŸŸ£ StreamTape: {src}")
        elif "mixdrop" in src.lower():
            print(f"  ğŸ”µ Mixdrop: {src}")
        else:
            print(f"  âšª Outro: {src}")
    
    return sources

def main():
    print("="*60)
    print("ğŸ” PARSE PROFUNDO DO MAXSERIES.ONE")
    print("="*60)
    
    results = {}
    
    # 1. Home
    results['home'] = parse_home()
    
    # 2. Filmes
    movies = parse_movies_page()
    results['movies'] = movies
    
    # 3. SÃ©ries
    series = parse_series_page()
    results['series'] = series
    
    # 4. Detalhes de um filme
    if movies and movies[0]['link']:
        movie_detail = parse_movie_detail(movies[0]['link'])
        results['movie_detail'] = movie_detail
    
    # 5. Detalhes de uma sÃ©rie
    if series and series[0]['link']:
        series_detail = parse_series_detail(series[0]['link'])
        results['series_detail'] = series_detail
        
        # 6. PlayerThree
        if series_detail.get('playerthree_urls'):
            playerthree_url = series_detail['playerthree_urls'][0]
            playerthree_data = parse_playerthree(playerthree_url)
            results['playerthree'] = playerthree_data
            
            # 7. EpisÃ³dio especÃ­fico
            # Usar um ID de episÃ³dio conhecido
            episode_id = "258444"  # ID que sabemos que funciona
            sources = parse_playerthree_episode(playerthree_url, episode_id)
            results['episode_sources'] = sources
    
    # Salvar resultados
    with open("maxseries_parse_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*60)
    print("âœ… PARSE COMPLETO!")
    print("="*60)
    print("\nArquivos gerados:")
    print("  - maxseries_home.html")
    print("  - maxseries_filmes.html")
    print("  - maxseries_series.html")
    print("  - maxseries_movie_detail.html")
    print("  - maxseries_series_detail.html")
    print("  - maxseries_playerthree.html")
    print("  - maxseries_episode_*.html")
    print("  - maxseries_parse_results.json")

if __name__ == "__main__":
    main()
